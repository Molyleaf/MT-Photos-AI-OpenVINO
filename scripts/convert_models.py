import argparse
from pathlib import Path
import logging
import sys
import torch
import openvino as ov
from transformers import AutoProcessor, AutoModel

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- ä¿®æ”¹ï¼šåŒ…è£…ç±»ç°åœ¨åŒ…å« ç¼–ç å™¨ + æŠ•å½±å±‚ ---
class VisionModelWrapper(torch.nn.Module):
    """
    åŒ…è£…è§†è§‰æ¨¡å‹åŠå…¶æŠ•å½±å±‚ï¼Œ
    ä½¿å…¶ forward æ—¶è¿”å› [batch_size, 1024] çš„æœ€ç»ˆæŠ•å½±ç‰¹å¾ã€‚
    """
    def __init__(self, vision_model, visual_projection):
        super().__init__()
        self.vision_model = vision_model
        self.visual_projection = visual_projection

    def forward(self, pixel_values):
        # 1. ç¼–ç å™¨è¾“å‡º (å¾—åˆ° [?, 1280] ç»´çš„ pooler_output)
        outputs = self.vision_model(pixel_values=pixel_values)
        pooled_output = outputs.pooler_output
        # 2. æŠ•å½±å±‚ (å°† [?, 1280] è½¬æ¢ä¸º [?, 1024])
        projected_output = self.visual_projection(pooled_output)
        return projected_output

class TextModelWrapper(torch.nn.Module):
    """
    åŒ…è£…æ–‡æœ¬æ¨¡å‹åŠå…¶æŠ•å½±å±‚ï¼Œ
    ä½¿å…¶ forward æ—¶è¿”å› [batch_size, 1024] çš„æœ€ç»ˆæŠ•å½±ç‰¹å¾ã€‚
    """
    def __init__(self, text_model, text_projection):
        super().__init__()
        self.text_model = text_model
        self.text_projection = text_projection

    def forward(self, input_ids):
        # 1. ç¼–ç å™¨è¾“å‡º (å¾—åˆ° [?, 1024] ç»´çš„ pooler_output)
        outputs = self.text_model(input_ids=input_ids)
        pooled_output = outputs.pooler_output
        # 2. æŠ•å½±å±‚ (å°† [?, 1024] è½¬æ¢ä¸º [?, 1024])
        projected_output = self.text_projection(pooled_output)
        return projected_output
# --- ç»“æŸä¿®æ”¹ ---


def convert_model_manual(output_dir_str: str):
    model_name = "BAAI/AltCLIP-m18"
    output_dir = Path(output_dir_str)

    logging.info(f"å¼€å§‹ä» '{model_name}' æ‰‹åŠ¨è½¬æ¢æ¨¡å‹...")
    logging.info(f"æ¨¡å‹å°†è¢«ä¿å­˜åˆ°: {output_dir}")

    output_dir.mkdir(parents=True, exist_ok=True)

    try:
        # --- 1. åŠ è½½å¹¶ä¿å­˜å¤„ç†å™¨ ---
        logging.info("åŠ è½½ Processor...")
        # trust_remote_code=True æ˜¯å¿…éœ€çš„
        processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
        processor.save_pretrained(output_dir)
        logging.info(f"Processor æ–‡ä»¶å·²ä¿å­˜åˆ° {output_dir}")

        # --- 2. åŠ è½½ PyTorch æ¨¡å‹ ---
        logging.info("åŠ è½½ PyTorch æ¨¡å‹ (trust_remote_code=True)...")
        pt_model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
        pt_model.eval() # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

        # --- ä¿®æ”¹ï¼šå®ä¾‹åŒ–åŒ…è£…æ¨¡å‹ (ä¼ å…¥ç¼–ç å™¨å’ŒæŠ•å½±å±‚) ---
        logging.info("åˆ›å»ºç”¨äºå¯¼å‡ºçš„åŒ…è£…æ¨¡å‹ (åŒ…å«æŠ•å½±å±‚)...")
        vision_model = VisionModelWrapper(
            pt_model.vision_model,
            pt_model.visual_projection
        )
        text_model = TextModelWrapper(
            pt_model.text_model,
            pt_model.text_projection
        )
        # --- ç»“æŸä¿®æ”¹ ---

        # --- 3. å‡†å¤‡ ONNX å¯¼å‡ºçš„è™šæ‹Ÿè¾“å…¥ ---

        # è§†è§‰æ¨¡å‹
        # (batch_size, num_channels, height, width)
        dummy_pixel_values = torch.randn(1, 3, 224, 224)

        # æ–‡æœ¬æ¨¡å‹
        # (batch_size, sequence_length)
        seq_len = processor.tokenizer.model_max_length
        vocab_size = processor.tokenizer.vocab_size
        dummy_input_ids = torch.randint(0, vocab_size, (1, seq_len))

        vision_onnx_path = output_dir / "vision_model.onnx"
        text_onnx_path = output_dir / "text_model.onnx"

        # --- 4. å¯¼å‡ºè§†è§‰æ¨¡å‹åˆ° ONNX ---
        logging.info("å¯¼å‡ºè§†è§‰æ¨¡å‹åˆ° ONNX...")
        torch.onnx.export(
            vision_model,            # <--- MODIFIED: ä½¿ç”¨æ–°çš„åŒ…è£…å™¨
            dummy_pixel_values,
            vision_onnx_path,
            input_names=["pixel_values"],
            output_names=["pooler_output"], # ç°åœ¨è¿™ä¸ªåç§°å¯¹åº”æ­£ç¡®çš„ [?, 1024] å¼ é‡
            dynamic_axes={"pixel_values": {0: "batch_size"}},
            opset_version=18  # ä¿æŒ opset 18
        )

        # --- 5. å¯¼å‡ºæ–‡æœ¬æ¨¡å‹åˆ° ONNX ---
        logging.info("å¯¼å‡ºæ–‡æœ¬æ¨¡å‹åˆ° ONNX...")
        torch.onnx.export(
            text_model,              # <--- MODIFIED: ä½¿ç”¨æ–°çš„åŒ…è£…å™¨
            dummy_input_ids,
            text_onnx_path,
            input_names=["input_ids"],
            output_names=["pooler_output"], # ç°åœ¨è¿™ä¸ªåç§°å¯¹åº”æ­£ç¡®çš„ [?, 1024] å¼ é‡
            dynamic_axes={"input_ids": {0: "batch_size"}},
            opset_version=18  # ä¿æŒ opset 18
        )

        # --- 6. å°† ONNX è½¬æ¢ä¸º OpenVINO ---
        logging.info("ä½¿ç”¨ OpenVINO è½¬æ¢ ONNX æ¨¡å‹...")
        core = ov.Core()

        ov_vision_model = ov.convert_model(vision_onnx_path)
        ov_text_model = ov.convert_model(text_onnx_path)

        # --- 7. ä¿å­˜ OpenVINO æ¨¡å‹ ---
        # ä¿å­˜ä¸ºä½ çš„éªŒè¯è„šæœ¬æœŸæœ›çš„ .xml/.bin æ–‡ä»¶
        vision_model_path = output_dir / "openvino_vision_model.xml"
        text_model_path = output_dir / "openvino_text_model.xml"

        ov.save_model(ov_vision_model, vision_model_path)
        ov.save_model(ov_text_model, text_model_path)

        logging.info(f"OpenVINO è§†è§‰æ¨¡å‹å·²ä¿å­˜åˆ°: {vision_model_path}")
        logging.info(f"OpenVINO æ–‡æœ¬æ¨¡å‹å·²ä¿å­˜åˆ°: {text_model_path}")

        # --- 8. æ¸…ç†ä¸´æ—¶çš„ ONNX æ–‡ä»¶ ---
        vision_onnx_path.unlink()
        text_onnx_path.unlink()
        logging.info("ä¸´æ—¶çš„ ONNX æ–‡ä»¶å·²åˆ é™¤ã€‚")

    except Exception as e:
        logging.error(f"æ‰‹åŠ¨æ¨¡å‹è½¬æ¢æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        sys.exit(1)

    # --- ä½ çš„éªŒè¯é€»è¾‘ (æ— éœ€æ›´æ”¹ï¼Œç°åœ¨åº”è¯¥ä¼šé€šè¿‡) ---
    logging.info("--- å¼€å§‹éªŒè¯è½¬æ¢åçš„æ¨¡å‹ ---")
    try:
        core = ov.Core()
        # éªŒè¯å¤„ç†å™¨æ˜¯å¦å·²æ­£ç¡®ä¿å­˜
        AutoProcessor.from_pretrained(output_dir)
        logging.info("âœ… éªŒè¯æˆåŠŸ: Processor åŠ è½½æ­£å¸¸ã€‚")

        vision_model_path = output_dir / "openvino_vision_model.xml"
        text_model_path = output_dir / "openvino_text_model.xml"

        if not vision_model_path.exists() or not text_model_path.exists():
            raise FileNotFoundError("é”™è¯¯: æ‰‹åŠ¨è½¬æ¢åæœªæ‰¾åˆ°é¢„æœŸçš„æ¨¡å‹æ–‡ä»¶ã€‚")

        vision_model = core.read_model(vision_model_path)
        # é‡å‘½åéªŒè¯æ—¥å¿—ä¸­çš„ "Vè§†è§‰æ¨¡å‹" ä¸º "è§†è§‰æ¨¡å‹"
        vision_output_shape = vision_model.output("pooler_output").get_partial_shape()
        logging.info(f"å·²åŠ è½½çš„è§†è§‰æ¨¡å‹ 'pooler_output' ç»´åº¦: {vision_output_shape}")
        if vision_output_shape.rank.get_length() != 2 or vision_output_shape[1].get_length() != 1024:
            logging.error(f"éªŒè¯å¤±è´¥: è§†è§‰æ¨¡å‹ç»´åº¦ä¸æ˜¯ 1024ï¼")
        else:
            logging.info("âœ… éªŒè¯æˆåŠŸ: è§†è§‰æ¨¡å‹ç»´åº¦æ­£ç¡® (1024)ã€‚")

        text_model = core.read_model(text_model_path)
        text_output_shape = text_model.output("pooler_output").get_partial_shape()
        logging.info(f"å·²åŠ è½½çš„æ–‡æœ¬æ¨¡å‹ 'pooler_output' ç»´åº¦: {text_output_shape}")
        if text_output_shape.rank.get_length() != 2 or text_output_shape[1].get_length() != 1024:
            logging.error(f"éªŒè¯å¤±è´¥: æ–‡æœ¬æ¨¡å‹ç»´åº¦ä¸æ˜¯ 1024ï¼")
        else:
            logging.info("âœ… éªŒè¯æˆåŠŸ: æ–‡æœ¬æ¨¡å‹ç»´åº¦æ­£ç¡® (1024)ã€‚")

        logging.info("ğŸ‰ å…¨éƒ¨è½¬æ¢å’ŒéªŒè¯æˆåŠŸå®Œæˆã€‚")

    except Exception as e:
        logging.error(f"éªŒè¯è½¬æ¢åçš„æ¨¡å‹æ—¶å‡ºé”™: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æ‰‹åŠ¨å°† Alt-CLIP æ¨¡å‹è½¬æ¢ä¸º OpenVINO æ ¼å¼ã€‚")
    project_root = Path(__file__).resolve().parent.parent
    default_output = project_root / "models" / "alt-clip" / "openvino"
    parser.add_argument("--output_dir", type=str, default=str(default_output), help="è½¬æ¢åæ¨¡å‹çš„ä¿å­˜ç›®å½•ã€‚")
    args = parser.parse_args()

    # è°ƒç”¨æ–°çš„æ‰‹åŠ¨è½¬æ¢å‡½æ•°
    convert_model_manual(args.output_dir)