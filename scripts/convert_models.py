# scripts/convert_qa_clip_openvino.py
import logging
import sys
from pathlib import Path

# ç¡®ä¿å·²å®‰è£…æ‰€éœ€åº“
try:
    import openvino as ov
    import torch
    import torch.nn as nn
    # æ ¸å¿ƒï¼šä½¿ç”¨ transformers åŠ è½½
    from transformers import AutoProcessor, AutoModel
except ImportError:
    logging.error("å¿…éœ€åº“æœªæ‰¾åˆ°ã€‚è¯·è¿è¡Œ: pip install openvino openvino-dev torch transformers")
    sys.exit(1)

# --- é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ç›®æ ‡æ¨¡å‹ (Hugging Face)
MODEL_ID = "TencentARC/QA-CLIP-ViT-L-14"
# ViT-L-14 çš„åŸç”Ÿç»´åº¦
NATIVE_DIMS = 768
# ä»æ¨¡å‹é…ç½®ä¸­è·å–çš„æ ‡å‡†è¾“å…¥
INPUT_RESOLUTION = 224
# QA-CLIP ä½¿ç”¨çš„ä¸Šä¸‹æ–‡é•¿åº¦ (ä¸ OpenAI CLIP ä¸€è‡´)
CONTEXT_LENGTH = 77

# --- åŠ¨æ€è·¯å¾„å®šä¹‰ ---
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent
# è¾“å‡ºåˆ° models/qa-clip/openvino ç›®å½•
OV_SAVE_PATH = PROJECT_ROOT / "models" / "qa-clip" / "openvino"
CACHE_PATH = PROJECT_ROOT / "cache"


# --- å®šä¹‰æ¨¡å‹åŒ…è£…å™¨ (é€‚é… Transformers/QA-CLIP ç»“æ„) ---

class VisionModelWrapper(nn.Module):
    """
    åŒ…è£…å™¨ï¼Œè®¿é—®åŸºç¡€ ViT (.vision_model) å’ŒæŠ•å½±å±‚ (.visual_projection)ã€‚
    é€‚é… transformers çš„ QA-CLIP (ChineseCLIPVisionModel) ç»“æ„ã€‚
    """
    def __init__(self, model):
        super().__init__()
        # 1. è·å–åŸºç¡€ ViT æ¨¡å‹ (åœ¨ .vision_model å†…éƒ¨)
        self.vision_model_base = model.vision_model
        # 2. è·å–é¡¶å±‚çš„æŠ•å½±å±‚
        self.visual_projection = model.visual_projection

    def forward(self, pixel_values):
        # è¿è¡ŒåŸºç¡€ ViT æ¨¡å‹ã€‚
        # å®ƒè¿”å› (last_hidden_state, pooler_output, ...)
        vision_outputs = self.vision_model_base(pixel_values=pixel_values)

        # è·å– [CLS] æ ‡è®°çš„æ± åŒ–è¾“å‡º (pooler_output, ç´¢å¼• 1)
        pooled_output = vision_outputs[1]

        # è¿è¡Œæœ€ç»ˆçš„æŠ•å½±å±‚
        image_embeds = self.visual_projection(pooled_output)
        return image_embeds

# ... VisionModelWrapper ä¿æŒä¸å˜ ...

class TextModelWrapper(nn.Module):
    """
    åŒ…è£…å™¨, è®¿é—®åŸºç¡€ BERT (.text_model) å’Œ *å¤–éƒ¨* çš„æŠ•å½±å±‚ (.text_projection)ã€‚
    é€‚é… transformers çš„ ChineseCLIPTextModel å’Œ ChineseCLIPModel ç»“æ„ã€‚
    """
    def __init__(self, model):
        super().__init__()

        # --- ä¿®æ­£ ---
        # ChineseCLIPTextModel æ²¡æœ‰ .transformer å±æ€§
        # å®ƒæœ¬èº«å°±æ˜¯æˆ‘ä»¬è¦è°ƒç”¨çš„åŸºç¡€æ¨¡å‹ã€‚
        self.text_model_base = model.text_model
        # --- ç»“æŸä¿®æ­£ ---

        # 2. è·å–ä½äºé¡¶å±‚ ChineseCLIPModel çš„ text_projection å±‚
        self.text_projection = model.text_projection

    def forward(self, input_ids, attention_mask):
        # è¿è¡ŒåŸºç¡€ BERT æ¨¡å‹ (ç°åœ¨æ˜¯ self.text_model_base = model.text_model)
        # å®ƒè¿”å› (last_hidden_state, pooler_output, ...)
        text_outputs = self.text_model_base(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # æˆ‘ä»¬çš„ä¸Šä¸€ä¸ªä¿®å¤ï¼ˆæ‰‹åŠ¨æ± åŒ–ï¼‰åœ¨è¿™é‡Œä»ç„¶æ˜¯æ­£ç¡®çš„ã€‚
        # 1. ä»ç´¢å¼• [0] è·å– last_hidden_state
        last_hidden_state = text_outputs[0]

        # 2. æ‰‹åŠ¨æ‰§è¡Œæ± åŒ–ï¼šæå– [CLS] æ ‡è®°çš„è¾“å‡º (åœ¨åºåˆ—ç´¢å¼• 0 å¤„)
        pooled_output = last_hidden_state[:, 0, :]

        # è¿è¡Œæœ€ç»ˆçš„æŠ•å½±å±‚
        text_embeds = self.text_projection(pooled_output)
        return text_embeds

# --- ç»“æŸå®šä¹‰åŒ…è£…å™¨ ---


def convert_models():
    """
    æ‰§è¡Œ Pytorch -> OpenVINO IR (FP16) çš„å®Œæ•´è½¬æ¢æµç¨‹ã€‚
    ä½¿ç”¨ transformers åŠ è½½ï¼Œé€šè¿‡ Wrapper åˆ†ç¦»åˆ†æ”¯ï¼Œç›´æ¥è½¬æ¢ä¸º OpenVINOã€‚
    """

    OV_SAVE_PATH.mkdir(parents=True, exist_ok=True)
    CACHE_PATH.mkdir(parents=True, exist_ok=True)

    logging.info(f"OpenVINO æ¨¡å‹å°†ä¿å­˜åˆ°: {OV_SAVE_PATH}")
    logging.info(f"Hugging Face æ¨¡å‹ç¼“å­˜å°†ä½äº: {CACHE_PATH}")

    core = ov.Core()

    try:
        # --- æ­¥éª¤ 1: ä» Hugging Face åŠ è½½æ¨¡å‹ ---
        logging.info(f"--- æ­¥éª¤ 1: æ­£åœ¨ä» Hugging Face ä¸‹è½½å’ŒåŠ è½½æ¨¡å‹: {MODEL_ID} ---")

        # 1.1 åŠ è½½ Processor (ç”¨äºç”Ÿæˆä¼ªè¾“å…¥)
        # æ³¨æ„ï¼šQA-CLIP çš„ processor (tokenizer) ä¸ cn_clip ä¸åŒ
        processor = AutoProcessor.from_pretrained(
            MODEL_ID,
            cache_dir=CACHE_PATH,
            use_fast=True
        )

        # 1.2 åŠ è½½å®Œæ•´çš„ AutoModel (ChineseCLIPModel)
        model = AutoModel.from_pretrained(
            MODEL_ID,
            cache_dir=CACHE_PATH
        )
        model.eval() # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

        # --- æ­¥éª¤ 2: è½¬æ¢ Vision (å›¾åƒ) æ¨¡å‹ ---
        logging.info("--- æ­¥éª¤ 2: è½¬æ¢ Vision æ¨¡å‹ (FP16) ---")
        ov_vision_path = OV_SAVE_PATH / "openvino_image_fp16.xml"

        # å®ä¾‹åŒ– Vision åŒ…è£…å™¨
        vision_wrapper = VisionModelWrapper(model)

        # åˆ›å»ºä¼ªå›¾åƒè¾“å…¥ (Tensor, åŒ¹é… VisionWrapper.forward)
        dummy_image_input = torch.randn(1, 3, INPUT_RESOLUTION, INPUT_RESOLUTION)

        logging.info(f"æ­£åœ¨è½¬æ¢ Vision åŒ…è£…å™¨ -> {ov_vision_path}")
        ov_vision_model = ov.convert_model(vision_wrapper, example_input=dummy_image_input)
        ov.save_model(ov_vision_model, ov_vision_path, compress_to_fp16=True)
        logging.info("Vision æ¨¡å‹è½¬æ¢æˆåŠŸã€‚")

        # --- æ­¥éª¤ 3: è½¬æ¢ Text (æ–‡æœ¬) æ¨¡å‹ ---
        logging.info("--- æ­¥éª¤ 3: è½¬æ¢ Text æ¨¡å‹ (FP16) ---")
        ov_text_path = OV_SAVE_PATH / "openvino_text_fp16.xml"

        # å®ä¾‹åŒ– Text åŒ…è£…å™¨
        text_wrapper = TextModelWrapper(model)

        # åˆ›å»ºä¼ªæ–‡æœ¬è¾“å…¥ (Dict, åŒ¹é… TextWrapper.forward)
        # æ³¨æ„ context_length ä½¿ç”¨ QA-CLIP çš„ 77
        dummy_text_input = {
            "input_ids": torch.randint(0, processor.tokenizer.vocab_size, (1, CONTEXT_LENGTH)),
            "attention_mask": torch.ones(1, CONTEXT_LENGTH, dtype=torch.long)
        }

        logging.info(f"æ­£åœ¨è½¬æ¢ Text åŒ…è£…å™¨ -> {ov_text_path}")
        # å…³é”®ï¼šæŒ‡å®š input_ids å’Œ attention_mask çš„åŠ¨æ€ç»´åº¦
        ov_text_model = ov.convert_model(text_wrapper, example_input=dummy_text_input)
        ov.save_model(ov_text_model, ov_text_path, compress_to_fp16=True)
        logging.info("Text æ¨¡å‹è½¬æ¢æˆåŠŸã€‚")

    except Exception as e:
        logging.error(f"æ¨¡å‹è½¬æ¢å¤±è´¥: {e}", exc_info=True)
        sys.exit(1)

    # --- æ­¥éª¤ 4: éªŒè¯è½¬æ¢åçš„ OpenVINO æ¨¡å‹ ---
    logging.info("--- æ­¥éª¤ 4: éªŒè¯ OpenVINO IR æ¨¡å‹ ---")
    try:
        # éªŒè¯è§†è§‰æ¨¡å‹
        vision_model_ov = core.read_model(ov_vision_path)
        vision_output = vision_model_ov.output(0)
        vision_dims = vision_output.get_partial_shape()[1].get_length()

        if vision_dims == NATIVE_DIMS:
            logging.info(f"âœ… è§†è§‰æ¨¡å‹ç»´åº¦éªŒè¯æˆåŠŸ: {vision_dims}d")
        else:
            raise RuntimeError(f"è§†è§‰æ¨¡å‹ç»´åº¦é”™è¯¯! é¢„æœŸ: {NATIVE_DIMS}, å¾—åˆ°: {vision_dims}")

        vision_inputs_count = len(vision_model_ov.inputs)
        if vision_inputs_count != 1:
            raise RuntimeError(f"è§†è§‰æ¨¡å‹è¾“å…¥æ•°é‡é”™è¯¯! é¢„æœŸ: 1, å¾—åˆ°: {vision_inputs_count}")
        logging.info(f"âœ… è§†è§‰æ¨¡å‹è¾“å…¥æ•°é‡éªŒè¯æˆåŠŸ: {vision_inputs_count}")

        # éªŒè¯æ–‡æœ¬æ¨¡å‹
        text_model_ov = core.read_model(ov_text_path)
        text_output = text_model_ov.output(0)
        text_dims = text_output.get_partial_shape()[1].get_length()

        if text_dims == NATIVE_DIMS:
            logging.info(f"âœ… æ–‡æœ¬æ¨¡å‹ç»´åº¦éªŒè¯æˆåŠŸ: {text_dims}d")
        else:
            raise RuntimeError(f"æ–‡æœ¬æ¨¡å‹ç»´åº¦é”™è¯¯! é¢„æœŸ: {NATIVE_DIMS}, å¾—åˆ°: {text_dims}")

        text_inputs_count = len(text_model_ov.inputs)
        # TextWrapper éœ€è¦ input_ids å’Œ attention_mask
        if text_inputs_count != 2:
            raise RuntimeError(f"æ–‡æœ¬æ¨¡å‹è¾“å…¥æ•°é‡é”™è¯¯! é¢„æœŸ: 2, å¾—åˆ°: {text_inputs_count}")
        logging.info(f"âœ… æ–‡æœ¬æ¨¡å‹è¾“å…¥æ•°é‡éªŒè¯æˆåŠŸ: {text_inputs_count}")

    except Exception as e:
        logging.error(f"æ¨¡å‹éªŒè¯å¤±è´¥: {e}", exc_info=True)
        sys.exit(1)

    logging.info(f"ğŸ‰ å…¨éƒ¨è½¬æ¢å’ŒéªŒè¯æˆåŠŸå®Œæˆã€‚æ¨¡å‹ä¿å­˜åœ¨: {OV_SAVE_PATH}")

if __name__ == "__main__":
    convert_models()